{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5d40a14-130e-493c-aabf-d15bdb8d6260",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "bs = 100\n",
    "# MNIST Dataset\n",
    "#train_dataset = datasets.MNIST(root='/root/data/', train=True, transform=transforms.ToTensor(), download=True)\n",
    "train_dataset = datasets.MNIST(root='/root/data/', train=True, transform=transforms.ToTensor(), download=False)\n",
    "test_dataset = datasets.MNIST(root='/root/data/', train=False, transform=transforms.ToTensor(), download=False)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=bs, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8d602728-ef97-4a1c-b8eb-7405e112eb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv1 = nn.Conv2d(dim, dim, 3, 1, 1)\n",
    "        self.conv2 = nn.Conv2d(dim, dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        tmp = self.relu(x)\n",
    "        tmp = self.conv1(tmp)\n",
    "        tmp = self.relu(tmp)\n",
    "        tmp = self.conv2(tmp)\n",
    "        return x + tmp\n",
    "\n",
    "\n",
    "class VQVAE(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, dim, n_embedding):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(nn.Conv2d(input_dim, dim, 4, 2, 1),\n",
    "                                     nn.ReLU(), nn.Conv2d(dim, dim, 4, 2, 1),\n",
    "                                     nn.ReLU(), nn.Conv2d(dim, dim, 3, 1, 1),\n",
    "                                     ResidualBlock(dim), ResidualBlock(dim))\n",
    "        self.vq_embedding = nn.Embedding(n_embedding, dim)\n",
    "        self.vq_embedding.weight.data.uniform_(-1.0 / n_embedding,\n",
    "                                               1.0 / n_embedding)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(dim, dim, 3, 1, 1),\n",
    "            ResidualBlock(dim), ResidualBlock(dim),\n",
    "            nn.ConvTranspose2d(dim, dim, 4, 2, 1), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(dim, input_dim, 4, 2, 1))\n",
    "        self.n_downsample = 2\n",
    "\n",
    "    def forward(self, x):\n",
    "        # encode\n",
    "        ze = self.encoder(x)\n",
    "\n",
    "        # ze: [N, C, H, W]\n",
    "        # embedding [K, C]\n",
    "        embedding = self.vq_embedding.weight.data\n",
    "        N, C, H, W = ze.shape\n",
    "        K, _ = embedding.shape\n",
    "        embedding_broadcast = embedding.reshape(1, K, C, 1, 1)\n",
    "        ze_broadcast = ze.reshape(N, 1, C, H, W)\n",
    "        distance = torch.sum((embedding_broadcast - ze_broadcast)**2, 2)\n",
    "        nearest_neighbor = torch.argmin(distance, 1)\n",
    "        zq = self.vq_embedding(nearest_neighbor).permute(0, 3, 1, 2)\n",
    "        decoder_input = ze + (zq - ze).detach()\n",
    "\n",
    "        # decode\n",
    "        x_hat = self.decoder(decoder_input)\n",
    "        return x_hat, ze, zq\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode(self, x):\n",
    "        ze = self.encoder(x)\n",
    "        embedding = self.vq_embedding.weight.data\n",
    "\n",
    "        # ze: [N, C, H, W]\n",
    "        # embedding [K, C]\n",
    "        N, C, H, W = ze.shape\n",
    "        K, _ = embedding.shape\n",
    "        embedding_broadcast = embedding.reshape(1, K, C, 1, 1)\n",
    "        ze_broadcast = ze.reshape(N, 1, C, H, W)\n",
    "        distance = torch.sum((embedding_broadcast - ze_broadcast)**2, 2)\n",
    "        nearest_neighbor = torch.argmin(distance, 1)\n",
    "        return nearest_neighbor\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def decode(self, discrete_latent):\n",
    "        zq = self.vq_embedding(discrete_latent).permute(0, 3, 1, 2)\n",
    "        x_hat = self.decoder(zq)\n",
    "        return x_hat\n",
    "\n",
    "    def get_latent_HW(self, input_shape):\n",
    "        C, H, W = input_shape\n",
    "        return (H // 2**self.n_downsample, W // 2**self.n_downsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "91979fbb-2603-4210-b7ee-9ede51c0c190",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskConv2d(nn.Module):\n",
    "\n",
    "    def __init__(self, conv_type, *args, **kwags):\n",
    "        super().__init__()\n",
    "        assert conv_type in ('A', 'B')\n",
    "        self.conv = nn.Conv2d(*args, **kwags)\n",
    "        H, W = self.conv.weight.shape[-2:]\n",
    "        mask = torch.zeros((H, W), dtype=torch.float32)\n",
    "        mask[0:H // 2] = 1\n",
    "        mask[H // 2, 0:W // 2] = 1\n",
    "        if conv_type == 'B':\n",
    "            mask[H // 2, W // 2] = 1\n",
    "        mask = mask.reshape((1, 1, H, W))\n",
    "        self.register_buffer('mask', mask, False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.conv.weight.data *= self.mask\n",
    "        conv_res = self.conv(x)\n",
    "        return conv_res\n",
    "    \n",
    "# Gated PixelCNN\n",
    "class VerticalMaskConv2d(nn.Module):\n",
    "\n",
    "    def __init__(self, *args, **kwags):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(*args, **kwags)\n",
    "        H, W = self.conv.weight.shape[-2:]\n",
    "        mask = torch.zeros((H, W), dtype=torch.float32)\n",
    "        mask[0:H // 2 + 1] = 1\n",
    "        mask = mask.reshape((1, 1, H, W))\n",
    "        self.register_buffer('mask', mask, False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.conv.weight.data *= self.mask\n",
    "        conv_res = self.conv(x)\n",
    "        return conv_res\n",
    "\n",
    "\n",
    "class HorizontalMaskConv2d(nn.Module):\n",
    "\n",
    "    def __init__(self, conv_type, *args, **kwags):\n",
    "        super().__init__()\n",
    "        assert conv_type in ('A', 'B')\n",
    "        self.conv = nn.Conv2d(*args, **kwags)\n",
    "        H, W = self.conv.weight.shape[-2:]\n",
    "        mask = torch.zeros((H, W), dtype=torch.float32)\n",
    "        mask[H // 2, 0:W // 2] = 1\n",
    "        if conv_type == 'B':\n",
    "            mask[H // 2, W // 2] = 1\n",
    "        mask = mask.reshape((1, 1, H, W))\n",
    "        self.register_buffer('mask', mask, False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.conv.weight.data *= self.mask\n",
    "        conv_res = self.conv(x)\n",
    "        return conv_res\n",
    "\n",
    "\n",
    "class GatedBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, conv_type, in_channels, p, bn=True):\n",
    "        super().__init__()\n",
    "        self.conv_type = conv_type\n",
    "        self.p = p\n",
    "        self.v_conv = VerticalMaskConv2d(in_channels, 2 * p, 3, 1, 1)\n",
    "        self.bn1 = nn.BatchNorm2d(2 * p) if bn else nn.Identity()\n",
    "        self.v_to_h_conv = nn.Conv2d(2 * p, 2 * p, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(2 * p) if bn else nn.Identity()\n",
    "        self.h_conv = HorizontalMaskConv2d(conv_type, in_channels, 2 * p, 3, 1, 1)\n",
    "        self.bn3 = nn.BatchNorm2d(2 * p) if bn else nn.Identity()\n",
    "        self.h_output_conv = nn.Conv2d(p, p, 1)\n",
    "        self.bn4 = nn.BatchNorm2d(p) if bn else nn.Identity()\n",
    "\n",
    "    def forward(self, v_input, h_input):\n",
    "        v = self.v_conv(v_input)\n",
    "        v = self.bn1(v)\n",
    "        v_to_h = v[:, :, 0:-1]\n",
    "        v_to_h = F.pad(v_to_h, (0, 0, 1, 0))\n",
    "        v_to_h = self.v_to_h_conv(v_to_h)\n",
    "        v_to_h = self.bn2(v_to_h)\n",
    "\n",
    "        v1, v2 = v[:, :self.p], v[:, self.p:]\n",
    "        v1 = torch.tanh(v1)\n",
    "        v2 = torch.sigmoid(v2)\n",
    "        v = v1 * v2\n",
    "\n",
    "        h = self.h_conv(h_input)\n",
    "        h = self.bn3(h)\n",
    "        h = h + v_to_h\n",
    "        h1, h2 = h[:, :self.p], h[:, self.p:]\n",
    "        h1 = torch.tanh(h1)\n",
    "        h2 = torch.sigmoid(h2)\n",
    "        h = h1 * h2\n",
    "        h = self.h_output_conv(h)\n",
    "        h = self.bn4(h)\n",
    "        if self.conv_type == 'B':\n",
    "            h = h + h_input\n",
    "        return v, h\n",
    "\n",
    "\n",
    "class GatedPixelCNN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_blocks, p, linear_dim, bn=True, color_level=256):\n",
    "        super().__init__()\n",
    "        self.block1 = GatedBlock('A', 1, p, bn)\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for _ in range(n_blocks):\n",
    "            self.blocks.append(GatedBlock('B', p, p, bn))\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear1 = nn.Conv2d(p, linear_dim, 1)\n",
    "        self.linear2 = nn.Conv2d(linear_dim, linear_dim, 1)\n",
    "        self.out = nn.Conv2d(linear_dim, color_level, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        v, h = self.block1(x, x)\n",
    "        for block in self.blocks:\n",
    "            v, h = block(v, h)\n",
    "        x = self.relu(h)\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "class PixelCNNWithEmbedding(GatedPixelCNN):\n",
    "\n",
    "    def __init__(self, n_blocks, p, linear_dim, bn=True, color_level=256):\n",
    "        super().__init__(n_blocks, p, linear_dim, bn, color_level)\n",
    "        self.embedding = nn.Embedding(color_level, p)\n",
    "        self.block1 = GatedBlock('A', p, p, bn)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.permute(0, 3, 1, 2).contiguous()\n",
    "        return super().forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "79734408-f2c1-41db-8efb-0e50669a1d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "n_epochs=250\n",
    "lr =1e-2\n",
    "\n",
    "def train_generative_model(device, vqvae, model):\n",
    "    vqvae.to(device)\n",
    "    vqvae.eval()\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    tic = time()\n",
    "    for e in range(n_epochs):\n",
    "        total_loss = 0\n",
    "        for batch_idx, (x, _) in enumerate(train_loader):\n",
    "            current_batch_size = x.shape[0]\n",
    "            with torch.no_grad():\n",
    "                x = x.to(device)\n",
    "                x = vqvae.encode(x)\n",
    "\n",
    "            predict_x = model(x)\n",
    "            loss = loss_fn(predict_x, x)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * current_batch_size\n",
    "        total_loss /= len(train_loader)\n",
    "        toc = time()\n",
    "        print(f'epoch {e} loss: {total_loss} elapsed {(toc - tic):.2f}s')\n",
    "    torch.save(model, '/root/new/VQVAE/pix/genera_model.pth')\n",
    "    print('Done')\n",
    "\n",
    "\n",
    "def sample_imgs(device, vqvae, gen_model):\n",
    "    vqvae = vqvae.to(device)\n",
    "    vqvae.eval()\n",
    "    gen_model = gen_model.to(device)\n",
    "    gen_model.eval()\n",
    "    \n",
    "    n_sample = 100\n",
    "    \n",
    "    C, H, W = 1, 28, 28\n",
    "    H, W = H // 2**2, W // 2**2\n",
    "    input_shape = (n_sample, H, W)\n",
    "    x = torch.zeros(input_shape).to(device).to(torch.long)\n",
    "    with torch.no_grad():\n",
    "        for i in range(H):\n",
    "            for j in range(W):\n",
    "                output = gen_model(x)\n",
    "                prob_dist = F.softmax(output[:, :, i, j], -1)\n",
    "                pixel = torch.multinomial(prob_dist, 1)\n",
    "                x[:, i, j] = pixel[:, 0]\n",
    "\n",
    "    imgs = vqvae.decode(x)\n",
    "\n",
    "    save_image(imgs, '/root/new/VQVAE/pix/pictures/genera.png', nrow=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f43c8ca-bc04-4c05-9929-af69f4e05933",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    device = 'cuda:0'\n",
    "    gen_model = PixelCNNWithEmbedding(n_blocks=15, p=128, linear_dim=32, bn=True, color_level=10)\n",
    "    vqvae = torch.load('/root/new/VQVAE/pix/model.pth', map_location=device)\n",
    "\n",
    "    train_generative_model(device = device, vqvae = vqvae, model = gen_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1e540bf9-fa2c-4f58-b342-b3efb99fabd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Sample VQVAE\n",
    "device = 'cuda:0'\n",
    "vqvae = torch.load('/root/new/VQVAE/pix/model.pth', map_location=device)\n",
    "gen_model = torch.load('/root/new/VQVAE/pix/genera_model.pth', map_location=device)\n",
    "sample_imgs(device=device, vqvae=vqvae, gen_model=gen_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1113eb9-534d-42c5-a0ef-ed16d1cd4721",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
