{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99311bd7-31cf-4701-98a6-4c935c1a4bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "root = '/root/data/CIFAR10/'\n",
    "\n",
    "# Load the CIFAR-10\n",
    "train_dataset = torchvision.datasets.CIFAR10(root=root, train=True, transform=transform, download=False)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root=root, train=False, transform=transform, download=False)\n",
    "\n",
    "bs = 100\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=bs, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=bs, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "716d7326-71cf-48ab-a194-2c5e0d5974f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv1 = nn.Conv2d(dim, dim, 3, 1, 1)\n",
    "        self.conv2 = nn.Conv2d(dim, dim, 1)\n",
    "        self.batchnorm2d = nn.BatchNorm2d(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        tmp = self.conv1(x)\n",
    "        tmp = self.batchnorm2d(tmp)\n",
    "        tmp = self.relu(tmp)\n",
    "        tmp = self.conv2(tmp)\n",
    "        tmp = self.batchnorm2d(tmp)\n",
    "        tmp = x + tmp\n",
    "        tmp = self.relu(tmp)\n",
    "        return tmp\n",
    "\n",
    "\n",
    "class VQVAE(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, n_embedding):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(nn.Conv2d(dim[0], dim[1], 4, 2, 1),\n",
    "                                     nn.ReLU(), nn.Conv2d(dim[1], dim[2], 4, 2, 1),\n",
    "                                     nn.ReLU(),\n",
    "                                     ResidualBlock(dim[2]), ResidualBlock(dim[2]))\n",
    "        self.vq_embedding = nn.Embedding(n_embedding, dim[2])\n",
    "        self.vq_embedding.weight.data.uniform_(-1.0 / n_embedding,\n",
    "                                               1.0 / n_embedding)\n",
    "        self.decoder = nn.Sequential(\n",
    "            ResidualBlock(dim[2]), ResidualBlock(dim[2]),\n",
    "            nn.ConvTranspose2d(dim[2], dim[1], 4, 2, 1), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(dim[1], dim[0], 4, 2, 1))\n",
    "        self.n_downsample = 2\n",
    "\n",
    "    def forward(self, x):\n",
    "        # encode\n",
    "        ze = self.encoder(x)\n",
    "\n",
    "        # ze: [N, C, H, W]\n",
    "        # embedding [K, C]\n",
    "        embedding = self.vq_embedding.weight.data\n",
    "        N, C, H, W = ze.shape\n",
    "        K, _ = embedding.shape\n",
    "        embedding_broadcast = embedding.reshape(1, K, C, 1, 1)\n",
    "        ze_broadcast = ze.reshape(N, 1, C, H, W)\n",
    "        distance = torch.sum((embedding_broadcast - ze_broadcast)**2, 2)\n",
    "        nearest_neighbor = torch.argmin(distance, 1)\n",
    "        # make C to the second dim\n",
    "        zq = self.vq_embedding(nearest_neighbor).permute(0, 3, 1, 2)\n",
    "        # stop gradient\n",
    "        decoder_input = ze + (zq - ze).detach()\n",
    "\n",
    "        # decode\n",
    "        x_hat = self.decoder(decoder_input)\n",
    "        return x_hat, ze, zq\n",
    "\n",
    "    def encode(self, x):\n",
    "        ze = self.encoder(x)\n",
    "        embedding = self.vq_embedding.weight.data\n",
    "\n",
    "        # ze: [N, C, H, W]\n",
    "        # embedding [K, C]\n",
    "        N, C, H, W = ze.shape\n",
    "        K, _ = embedding.shape\n",
    "        embedding_broadcast = embedding.reshape(1, K, C, 1, 1)\n",
    "        ze_broadcast = ze.reshape(N, 1, C, H, W)\n",
    "        distance = torch.sum((embedding_broadcast - ze_broadcast)**2, 2)\n",
    "        nearest_neighbor = torch.argmin(distance, 1)\n",
    "        return nearest_neighbor\n",
    "\n",
    "    def decode(self, discrete_latent):\n",
    "        zq = self.vq_embedding(discrete_latent).permute(0, 3, 1, 2)\n",
    "        x_hat = self.decoder(zq)\n",
    "        return x_hat\n",
    "\n",
    "    # Shape: [C, H, W]\n",
    "    def get_latent_HW(self, input_shape):\n",
    "        C, H, W = input_shape\n",
    "        return (H // 2**self.n_downsample, W // 2**self.n_downsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd4f9559-403f-447b-852a-5bb6e7a5c863",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskConv2d(nn.Module):\n",
    "\n",
    "    def __init__(self, conv_type, *args, **kwags):\n",
    "        super().__init__()\n",
    "        assert conv_type in ('A', 'B')\n",
    "        self.conv = nn.Conv2d(*args, **kwags)\n",
    "        H, W = self.conv.weight.shape[-2:]\n",
    "        mask = torch.zeros((H, W), dtype=torch.float32)\n",
    "        mask[0:H // 2] = 1\n",
    "        mask[H // 2, 0:W // 2] = 1\n",
    "        if conv_type == 'B':\n",
    "            mask[H // 2, W // 2] = 1\n",
    "        mask = mask.reshape((1, 1, H, W))\n",
    "        self.register_buffer('mask', mask, False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.conv.weight.data *= self.mask\n",
    "        conv_res = self.conv(x)\n",
    "        return conv_res\n",
    "    \n",
    "# Gated PixelCNN\n",
    "class VerticalMaskConv2d(nn.Module):\n",
    "\n",
    "    def __init__(self, *args, **kwags):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(*args, **kwags)\n",
    "        H, W = self.conv.weight.shape[-2:]\n",
    "        mask = torch.zeros((H, W), dtype=torch.float32)\n",
    "        mask[0:H // 2 + 1] = 1\n",
    "        mask = mask.reshape((1, 1, H, W))\n",
    "        self.register_buffer('mask', mask, False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.conv.weight.data *= self.mask\n",
    "        conv_res = self.conv(x)\n",
    "        return conv_res\n",
    "\n",
    "\n",
    "class HorizontalMaskConv2d(nn.Module):\n",
    "\n",
    "    def __init__(self, conv_type, *args, **kwags):\n",
    "        super().__init__()\n",
    "        assert conv_type in ('A', 'B')\n",
    "        self.conv = nn.Conv2d(*args, **kwags)\n",
    "        H, W = self.conv.weight.shape[-2:]\n",
    "        mask = torch.zeros((H, W), dtype=torch.float32)\n",
    "        mask[H // 2, 0:W // 2] = 1\n",
    "        if conv_type == 'B':\n",
    "            mask[H // 2, W // 2] = 1\n",
    "        mask = mask.reshape((1, 1, H, W))\n",
    "        self.register_buffer('mask', mask, False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.conv.weight.data *= self.mask\n",
    "        conv_res = self.conv(x)\n",
    "        return conv_res\n",
    "\n",
    "\n",
    "class GatedBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, conv_type, in_channels, p, bn=True):\n",
    "        super().__init__()\n",
    "        self.conv_type = conv_type\n",
    "        self.p = p\n",
    "        self.v_conv = VerticalMaskConv2d(in_channels, 2 * p, 3, 1, 1)\n",
    "        self.bn1 = nn.BatchNorm2d(2 * p) if bn else nn.Identity()\n",
    "        self.v_to_h_conv = nn.Conv2d(2 * p, 2 * p, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(2 * p) if bn else nn.Identity()\n",
    "        self.h_conv = HorizontalMaskConv2d(conv_type, in_channels, 2 * p, 3, 1, 1)\n",
    "        self.bn3 = nn.BatchNorm2d(2 * p) if bn else nn.Identity()\n",
    "        self.h_output_conv = nn.Conv2d(p, p, 1)\n",
    "        self.bn4 = nn.BatchNorm2d(p) if bn else nn.Identity()\n",
    "\n",
    "    def forward(self, v_input, h_input):\n",
    "        v = self.v_conv(v_input)\n",
    "        v = self.bn1(v)\n",
    "        v_to_h = v[:, :, 0:-1]\n",
    "        v_to_h = F.pad(v_to_h, (0, 0, 1, 0))\n",
    "        v_to_h = self.v_to_h_conv(v_to_h)\n",
    "        v_to_h = self.bn2(v_to_h)\n",
    "\n",
    "        v1, v2 = v[:, :self.p], v[:, self.p:]\n",
    "        v1 = torch.tanh(v1)\n",
    "        v2 = torch.sigmoid(v2)\n",
    "        v = v1 * v2\n",
    "\n",
    "        h = self.h_conv(h_input)\n",
    "        h = self.bn3(h)\n",
    "        h = h + v_to_h\n",
    "        h1, h2 = h[:, :self.p], h[:, self.p:]\n",
    "        h1 = torch.tanh(h1)\n",
    "        h2 = torch.sigmoid(h2)\n",
    "        h = h1 * h2\n",
    "        h = self.h_output_conv(h)\n",
    "        h = self.bn4(h)\n",
    "        if self.conv_type == 'B':\n",
    "            h = h + h_input\n",
    "        return v, h\n",
    "\n",
    "\n",
    "class GatedPixelCNN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_blocks, p, linear_dim, bn=True, color_level=256):\n",
    "        super().__init__()\n",
    "        self.block1 = GatedBlock('A', 3, p, bn)\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for _ in range(n_blocks):\n",
    "            self.blocks.append(GatedBlock('B', p, p, bn))\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear1 = nn.Conv2d(p, linear_dim, 1)\n",
    "        self.linear2 = nn.Conv2d(linear_dim, linear_dim, 1)\n",
    "        self.out = nn.Conv2d(linear_dim, color_level, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        v, h = self.block1(x, x)\n",
    "        for block in self.blocks:\n",
    "            v, h = block(v, h)\n",
    "        x = self.relu(h)\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "class PixelCNNWithEmbedding(GatedPixelCNN):\n",
    "\n",
    "    def __init__(self, n_blocks, p, linear_dim, bn=True, color_level=256):\n",
    "        super().__init__(n_blocks, p, linear_dim, bn, color_level)\n",
    "        self.embedding = nn.Embedding(color_level, p)\n",
    "        self.block1 = GatedBlock('A', p, p, bn)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.permute(0, 3, 1, 2).contiguous()\n",
    "        return super().forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e73feb9-0c08-4d49-9f28-92211d620592",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "n_epochs= 1000\n",
    "lr =1e-2\n",
    "\n",
    "def train_generative_model(device, vqvae, model, is_continue, optimizer):\n",
    "    vqvae.to(device)\n",
    "    vqvae.eval()\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    if is_continue == False:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    tic = time()\n",
    "    for e in range(n_epochs):\n",
    "        total_loss = 0\n",
    "        for batch_idx, (x, _) in enumerate(train_loader):\n",
    "            current_batch_size = x.shape[0]\n",
    "            with torch.no_grad():\n",
    "                x = x.to(device)\n",
    "                x = vqvae.encode(x)\n",
    "\n",
    "            predict_x = model(x)\n",
    "            loss = loss_fn(predict_x, x)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * current_batch_size\n",
    "        total_loss /= len(train_loader)\n",
    "        toc = time()\n",
    "        print(f'epoch {e} loss: {total_loss} elapsed {(toc - tic):.2f}s')\n",
    "    #torch.save(model, '/root/fornewdata/CIFAR/VQVAE/genera_model.pth')\n",
    "    torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),}, \n",
    "    '/root/fornewdata/CIFAR/VQVAE/model_and_optimizer.pth')\n",
    "    print('Done')\n",
    "\n",
    "\n",
    "def sample_imgs(device, vqvae, gen_model):\n",
    "    vqvae = vqvae.to(device)\n",
    "    vqvae.eval()\n",
    "    gen_model = gen_model.to(device)\n",
    "    gen_model.eval()\n",
    "    \n",
    "    n_sample = 25\n",
    "    \n",
    "    C, H, W = 3, 32, 32\n",
    "    H, W = H // 2**2, W // 2**2\n",
    "    input_shape = (n_sample, H, W)\n",
    "    x = torch.zeros(input_shape).to(device).to(torch.long)\n",
    "    with torch.no_grad():\n",
    "        for i in range(H):\n",
    "            for j in range(W):\n",
    "                output = gen_model(x)\n",
    "                prob_dist = F.softmax(output[:, :, i, j], -1)\n",
    "                pixel = torch.multinomial(prob_dist, 1)\n",
    "                x[:, i, j] = pixel[:, 0]\n",
    "\n",
    "    imgs = vqvae.decode(x)\n",
    "    resized_image = torchvision.transforms.Resize((50, 50))(imgs)\n",
    "    save_image(resized_image, '/root/fornewdata/CIFAR/VQVAE/pictures/genera.png', nrow=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b612b0-4080-4732-8407-5fd5e226cc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    device = 'cuda:0'\n",
    "    vqvae = torch.load('/root/fornewdata/CIFAR/VQVAE/model.pth', map_location=device)\n",
    "    \n",
    "    gen_model = PixelCNNWithEmbedding(n_blocks=15, p=128, linear_dim=256, bn=True, color_level=512).to(device)\n",
    "    optimizer = torch.optim.Adam(gen_model.parameters(), lr)\n",
    "    \n",
    "    '''\n",
    "    checkpoint = torch.load('/root/fornewdata/CIFAR/VQVAE/model_and_optimizer.pth')\n",
    "    gen_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    '''\n",
    "    \n",
    "    train_generative_model(device = device, vqvae = vqvae, model = gen_model, is_continue = False, optimizer = optimizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9521e4f6-37b5-4030-a624-3c4bf9223d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Sample VQVAE\n",
    "from torchvision.utils import save_image\n",
    "device = 'cpu'\n",
    "vqvae = torch.load('/root/fornewdata/CIFAR/VQVAE/model.pth', map_location=device)\n",
    "#gen_model = torch.load('/root/fornewdata/CIFAR/VQVAE/genera_model.pth', map_location=device)\n",
    "gen_model = PixelCNNWithEmbedding(n_blocks=15, p=128, linear_dim=256, bn=True, color_level=512).to(device)\n",
    "checkpoint = torch.load('/root/fornewdata/CIFAR/VQVAE/model_and_optimizer.pth')\n",
    "gen_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "sample_imgs(device=device, vqvae=vqvae, gen_model=gen_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f2ea34-fbf3-45f4-bc38-6160c76e0bbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
